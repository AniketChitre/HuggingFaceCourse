# Ce fac dacă dataset-ul meu nu este pe Hub?[[what-if-my-dataset-isnt-on-the-hub]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[  
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/ro/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/ro/chapter5/section2.ipynb"},  
]} />

Ai învățat să folosești [Hugging Face Hub](https://huggingface.co/datasets) pentru a descărca dataseturi, dar vei găsi adesea că lucrați cu date care sunt stocate fie pe laptopul dumneavoastră, fie pe un server. În această secțiune vă vom arăta cum poate fi utilizat 🤗 Datasets pentru a încărca dataseturi care nu sunt disponibile pe Hugging Face Hub.
<Youtube id="HyQgpJTkRdE"/>

## Lucrând cu dataseturi locale și remote[[working-with-local-and-remote-datasets]]

🤗 Datasets oferă loading scripts pentru a gestiona încărcarea dataseturilor locale și remote. Suportă mai multe data formats , cum ar fi:

|    Data format     | Loading script |                         Example                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="my_file.csv")`     |
|     Text files     |     `text`     |    `load_dataset("text", data_files="my_file.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="my_file.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

As shown in the table, for each data format we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files` argument that specifies the path to one or more files. Let's start by loading a dataset from local files; later we'll see how to do the same with remote files.

## Loading a local dataset[[loading-a-local-dataset]]

For this example we'll use the [SQuAD-it dataset](https://github.com/crux82/squad-it/), which is a large-scale dataset for question answering in Italian.

The training and test splits are hosted on GitHub, so we can download them with a simple `wget` command:

În tabelele de mai sus, pentru fiecare format de date trebuie doar să specificăm tipul scriptului de încărcare din funcția `load_dataset()`, alături de un argument `data_files` care specifică calea către un sau mai multe fișiere. Începem cu încărcarea datasetului din fișierele locale; apoi vom vedea cum să faceți același lucru cu fișierele remote.

## Încărcarea unui dataset local[[loading-a-local-dataset]]

În acest exemplu, vom folosi [datasetul SQuAD-it](https://github.com/crux82/squad-it/), care este un large-scale dataset pentru întrebări și răspunsuri în italiană.

Spliturile de training și test sunt disponibile pe GitHub, deci putem descarca cu o comanda `wget`:
```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

This will download two compressed files called *SQuAD_it-train.json.gz* and *SQuAD_it-test.json.gz*, which we can decompress with the Linux `gzip` command:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```



Putem observa că fișierele comprimate au fost înlocuite cu _SQuAD_it-train.json_ și _SQuAD_it-test.json_, iar datele sunt stocate în formatul JSON.

<Tip>
✎ Dacă vă întrebați de ce există un caracter `!` în comenziile shell din exemplu, acest lucru se întâmplă pentru că le-am rulat în cadrul unui jupyter notebook. Simplu să ștergeți prefixul dacă doriți să descărcați și să faceți unzip datasetului într-un terminal.
</Tip>

Să încarcăm acum un fișier JSON cu funcția `load_dataset()`. Ne trebuie doar să știm dacă ne confruntăm cu un JSON obișnuit (similar cu un nested dictionary) sau JSON Lines (lines-separated JSON). Cum multe dataset-uri pentru întrebări și răspunsuri, SQuAD-it folosește formatul nested, în care toate textele sunt stocate într-un câmp numit `data`. Acest lucru ne permite să încărcăm datasetul specificând argumentul `field` astfel:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

By default, loading local files creates a `DatasetDict` object with a `train` split. We can see this by inspecting the `squad_it_dataset` object:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Acest lucru ne arată numărul de rânduri și numele coloanelor asociate cu setul de antrenare. Putem vizualiza un exemplu prin a indexa în `train` split astfel:
```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si è verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

Excelent, am reușit să încarcăm datasetul nostru local! Dar, deși acest lucru a funcționat pentru setul de antrenare, ceea ce dorim cu adevărat este să includem ambele splituri, `train` și `test`, într-un singur obiect `DatasetDict` astfel încât să putem aplica funcțiile `Dataset.map()` asupra ambelor splituri în același timp. Pentru a face acest lucru, putem furniza un dicționar pentru argumentul `data_files` care va face maps fiecărui spilt name cu fișierul asociat acelui split:

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Astfel obținem exact ceea ce am dori. Acum putem aplica diverse tehnici de preprocesare pentru a curăța datele, să facem tokenize recenziilor și așa mai departe.

<Tip>

Argumentul `data_files` al funcției `load_dataset()` este destul de flexibil și poate fi fie un singur file paths, o listă de file paths, sau un dicționar care face maps split nameurilor cu file pathurile. De asemenea putem să folosim glob files care se potrivest unui model specificat conform regulilor folosite de Unix shell (de exemplu putem să facem glob tuturor fișierelor JSON într-un folder ca un singur split setând `data_files="*.json"`). Pentru mai multe detalii consultați [documentația](https://huggingface.co/docs/datasets/loading#local-and-remote-files) 🤗 Datasets.

</Tip>

Scripturile de încărcare din 🤗 Datasets suportă automat decomprimarea fișierelor de intrare, astfel putem să evităm folosirea `gzip` arătând argumentul `data_files` direct către fișierele comprimate:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Acest lucru poate fi util dacă nu dorim să manualizăm dezcomprimarea mai multe fișiere GZIP. Automatic decompression se aplică și altor formate precum ZIP și TAR, astfel putem să indicăm `data_files` către fișierele comprimate și suntem gata!

Acum că știm cum să încărcăm fișierele locale pe laptop sau desktop, să vedem cum să încarcăm fișiere remote.

## Încărcarea unui dataset remote[[loading-a-remote-dataset]]

Dacă lucrați ca Data Scientist sau programatori într-o companie, există șanse mari că dataseturile pe care doriți să le analizați sunt stocate pe un anumit server. Din fericire, încărcarea fișierelor remote este la fel de simplă ca încărcarea celor locale! În schimb putem să oferim pathul la fișiere locale, noi oferim argumentului `data_files` de la metoda `load_dataset()` către una sau mai multe adrese URL unde sunt stocate fișierele remote. De exemplu pentru datasetul SQuAD-it găzduit pe GitHub, putem să oferitum argumentului `data_files`, URL-urile _SQuAD_it-*.json.gz_ astfel:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Astfel obținem același obiect `DatasetDict` ca mai sus, dar economisim timp, deoarece nu descărcăm și decomprimăm fișierele _SQuAD_it-*.json.gz_. Asta încheie aventura noastră în diversele modalități de încarcare a dataseturilor care nu sunt găzduite pe Hugging Face Hub. Acum că am avem un dataset la dispoziție, hai să îl prelucrăm!

<Tip>

✏️ **Încearcă!** Alegeți alt dataset găzduit pe GitHub sau [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) și încercați să îl încărcați atât local cât și remote folosind tehniciile introduse mai sus. Pentru puncte bonus, încercați să încărcați un dataset care este stocat în format CSV sau text (consultați [documentația](https://huggingface.co/docs/datasets/loading#local-and-remote-files) pentru detalii suplimentare despre aceste formate).

</Tip>




