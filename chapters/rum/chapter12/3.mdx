# Înțelegerea Lucrării DeepSeek R1

Acest capitol este un curs intensiv de citire a unei lucrări. Vom parcurge lucrarea în termeni simpli, și apoi vom descompune conceptele cheie și concluziile.

DeepSeek R1 reprezintă un progres semnificativ în antrenarea modelelor de limbaj, în special în dezvoltarea capacităților de raționament prin învățarea prin întărire. Lucrarea introduce un nou algoritm de învățare prin întărire numit Optimizarea Relativă a Politicii de Grup (GRPO).

![DeepSeek R1 Overview](https://huggingface.co/reasoning-course/images/resolve/main/grpo/4.png)

În următorul capitol, vom construi pe această cunoaștere și vom implementa GRPO în practică.

Obiectivul inițial al lucrării a fost să exploreze dacă învățarea prin întărire pură ar putea dezvolta capacități de raționament fără ajustarea fină supervizată.

<Tip>

Până la acel moment, toate LLM-urile populare necesitau o anumită ajustare fină supervizată, pe care am explorat-o în [capitolul 11](/course/chapter11/1).

</Tip>

## Momentul Revoluționar 'Aha'

![The 'Aha Moment'](https://huggingface.co/reasoning-course/images/resolve/main/grpo/9.png)

Una dintre cele mai remarcabile descoperiri în antrenarea R1-Zero a fost apariția unui fenomen cunoscut ca "Momentul Aha." Acest fenomen este oarecum similar cu modul în care oamenii experimentează realizări bruște în timpul rezolvării problemelor. Iată cum funcționează:

1. Încercare Inițială: Modelul face o încercare inițială de a rezolva o problemă
2. Recunoaștere: Recunoaște erori potențiale sau inconsistențe
3. Auto-Corecție: Își ajustează abordarea bazat pe această recunoaștere
4. Explicație: Poate explica de ce noua abordare este mai bună

Această descoperire rezonează cu cursanții și se simte ca un moment "Eureka". Demonstrează învățarea mai degrabă decât memorarea simplă, așa că să ne luăm un moment să ne imaginăm cum se simte să ai un moment "Aha".

De exemplu, imaginează-ți că încerci să rezolvi un puzzle:
- Prima încercare: "Această piesă ar trebui să meargă aici bazat pe culoare"
- Recunoaștere: "Dar stai, forma nu se potrivește destul de bine"
- Corecție: "Ah, de fapt merge acolo"
- Explicație: "Pentru că atât culoarea cât și modelul de formă se potrivesc în această poziție"

Această abilitate a apărut natural din antrenamentul RL, fără a fi programată explicit, demonstrând învățarea mai degrabă decât memorarea simplă a unui proces din datele de antrenare.

Cel mai ușor mod de a înțelege momentul 'Aha' este să-l vezi în acțiune. Să aruncăm o privire la un exemplu. În chat-ul de mai jos, întrebăm modelul să rezolve o problemă și UI-ul arată procesul de gândire al modelului pe măsură ce rezolvă problema.

<iframe
    src="https://reasoning-course-deepseek-ai-deepseek-r1-distill-0f5fad4.hf.space/"
	frameborder="0"
	width="850"
	height="450"
></iframe>

Dacă vrei să încerci Deepseek's R1, poți de asemenea să consulți [Hugging Chat](https://huggingface.co/chat/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B).

## Procesul de Antrenare

Antrenarea R1 a fost un proces cu mai multe faze. Să descompunem fazele și inovațiile cheie în fiecare fază.

Procesul final rezultă în două modele:
- DeepSeek-R1-Zero: Un model antrenat pur folosind învățarea prin întărire.
- DeepSeek-R1: Un model care construiește pe fundația DeepSeek-R1-Zero și adaugă ajustare fină supervizată.

| Caracteristică | DeepSeek-R1-Zero | DeepSeek-R1 |
|---------|------------------|--------------|
| Abordarea de Antrenare | RL Pur | Multi-fază (SFT + RL) |
| Ajustare Fină | Niciuna | Ajustare fină supervizată |
| Capacitate de Raționament | Emergentă | Îmbunătățită |
| Performanța AIME | 71.0% | 79.8% |
| Caracteristici Cheie | Raționament puternic dar probleme de lizibilitate | Consistență și lizibilitate mai bună a limbajului |

În timp ce DeepSeek-R1-Zero demonstrează potențialul învățării prin întărire pure pentru dezvoltarea capacităților de raționament, DeepSeek-R1 construiește pe această fundație cu o abordare mai echilibrată care prioritizează atât performanța de raționament cât și utilizabilitatea.

Procesul de antrenare implică patru faze:

1. Faza de Pornire la Rece
2. Faza RL de Raționament
3. Faza de Eșantionare prin Respingere
4. Faza RL Diversă

Să descompunem fiecare fază:

### Faza de Pornire la Rece (Fundația Calității)

![Cold Start Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/5.png)

Această fază este concepută pentru a stabili o fundație solidă pentru lizibilitatea și calitatea răspunsurilor modelului. Folosește un set de date mic de eșantioane de înaltă calitate de la R1-Zero pentru a ajusta fin modelul V3-Base. Începând cu modelul DeepSeek-V3-Base, echipa a folosit mii de eșantioane validate, de înaltă calitate de la R1-Zero pentru ajustarea fină supervizată. Această abordare inovatoare folosește un set de date mic dar de înaltă calitate pentru a stabili o lizibilitate de bază puternică și calitatea răspunsurilor.

### Faza RL de Raționament (Construirea Capacităților)

![Reasoning RL Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/6.png)

Faza RL de Raționament se concentrează pe dezvoltarea capacităților de raționament de bază în domenii incluzând matematica, codarea, știința și logica. Această fază folosește învățarea prin întărire bazată pe reguli, cu recompense direct legate de corectitudinea soluției.

Crucial, toate sarcinile din această fază sunt 'verificabile' deci putem verifica dacă răspunsul modelului este corect sau nu. De exemplu, în cazul matematicii, putem verifica dacă răspunsul modelului este corect folosind un rezolvitor matematic.

Ceea ce face această fază deosebit de inovatoare este abordarea sa de optimizare directă care elimină nevoia unui model de recompensă separat, simplificând procesul de antrenare.

### Faza de Eșantionare prin Respingere (Controlul Calității)

![Rejection Sampling Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/7.png)

În timpul Fazei de Eșantionare prin Respingere, modelul generează eșantioane care sunt apoi filtrate printr-un proces de control al calității. DeepSeek-V3 servește ca judecător al calității, evaluând rezultatele într-un domeniu larg care se extinde dincolo de sarcinile pure de raționament. Datele filtrate sunt apoi folosite pentru ajustarea fină supervizată. Inovația acestei faze constă în abilitatea sa de a combina multiple semnale de calitate pentru a asigura rezultate de înalt standard.

### Faza RL Diversă (Alinierea Largă)

![Diverse RL Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/8.png)

Faza finală RL Diversă abordează multiple tipuri de sarcini folosind o abordare hibridă sofisticată. Pentru sarcinile deterministe, folosește recompense bazate pe reguli, în timp ce sarcinile subiective sunt evaluate prin feedback LLM. Această fază își propune să atingă alinierea preferințelor umane prin abordarea sa inovatoare de recompense hibride, combinând precizia sistemelor bazate pe reguli cu flexibilitatea evaluării modelelor de limbaj.

## Algoritmul: Optimizarea Relativă a Politicii de Grup (GRPO)

Acum că avem o înțelegere bună a procesului de antrenare, să privim algoritmul care a fost folosit pentru a antrena modelul.

Autorii descriu GRPO ca o descoperire în ajustarea fină a modelelor:

![GRPO Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/10.png)

Noutatea GRPO constă în capacitatea sa de a "optimiza direct pentru rectificarea preferințelor." Aceasta semnifică o rută mai directă și eficientă către alinierea modelului cu rezultatele dorite, contrastând cu algoritmii tradiționali de Învățare prin Întărire precum PPO. Să descompunem cum funcționează GRPO prin cele trei componente principale ale sale.

### Formarea Grupurilor: Crearea de Multiple Soluții

Primul pas în GRPO este remarcabil de intuitiv - este similar cu modul în care un student ar putea rezolva o problemă dificilă încercând multiple abordări. Când primește o comandă, modelul nu generează doar un răspuns; în schimb, creează multiple încercări de a rezolva aceeași problemă (de obicei 4, 8 sau 16 încercări diferite).

Imaginează-ți că înveți un model să rezolve probleme de matematică. Pentru o întrebare despre numărarea puilor de pe o fermă, modelul ar putea genera mai multe soluții diferite:
- O soluție ar putea descompune problema pas cu pas: prima numărarea puilor totali, apoi scăderea cocoșilor, și în final contabilizarea găinilor care nu produc ouă
- Altă soluție ar putea folosi o abordare diferită dar la fel de validă
- Unele încercări ar putea conține greșeli sau soluții mai puțin eficiente

Toate aceste încercări sunt păstrate împreună ca grup, aproape ca să ai mai mulți studenti cu soluții de comparat și din care să înveți.

![Group Formation](https://huggingface.co/reasoning-course/images/resolve/main/grpo/11.jpg)

### Învățarea Preferințelor: Înțelegerea Ce Face o Soluție Bună

Aici este unde GRPO cu adevărat strălucește în simplitatea sa. Spre deosebire de alte metode pentru RLHF care întotdeauna necesită un model de recompensă separat pentru a prezice cât de bună ar putea fi o soluție, GRPO poate folosi orice funcție sau model pentru a evalua calitatea unei soluții. De exemplu, am putea folosi o funcție de lungime pentru a recompensa răspunsuri mai scurte sau un rezolvitor matematic pentru a recompensa soluții matematice precise.

Procesul de evaluare privește la diverse aspecte ale fiecărei soluții:
- Este răspunsul final corect?
- A urmat soluția formatarea corespunzătoare (cum ar fi folosirea tag-urilor XML corecte)?
- Se potrivește raționamentul cu răspunsul oferit?

Ceea ce face această abordare deosebit de inteligentă este modul în care gestionează evaluarea. În loc să dea doar scoruri absolute, GRPO normalizează recompensele în cadrul fiecărui grup. Folosește o formulă simplă dar eficientă pentru estimarea avantajului relativ al grupului:

```
Avantaj = (recompensă - mean(recompense_grup)) / std(recompense_grup)
```

![Preference Learning](https://huggingface.co/reasoning-course/images/resolve/main/grpo/12.jpg)

Această normalizare este ca să notezi pe o curbă, dar pentru AI. Ajută modelul să înțeleagă care soluții din cadrul grupului au fost mai bune sau mai rele comparativ cu colegii lor, mai degrabă decât să se uite doar la scoruri absolute.

### Optimizarea: Învățarea din Experiență

Pasul final este unde GRPO îi învață modelul să se îmbunătățească bazat pe ceea ce a învățat din evaluarea grupului de soluții. Acest proces este atât puternic cât și stabil, folosind două principii principale:

1. Încurajează modelul să producă mai multe soluții ca cele de succes în timp ce se îndepărtează de abordările mai puțin eficiente
2. Include un mecanism de siguranță (numit penalitate de divergență KL) care previne modelul să se schimbe prea drastic dintr-o dată

Această abordare se dovedește mai stabilă decât metodele tradiționale pentru că:
- Se uită la multiple soluții împreună mai degrabă decât să compare doar două pe rând
- Normalizarea bazată pe grup ajută la prevenirea problemelor cu scalarea recompenselor
- Penalitatea KL acționează ca o plasă de siguranță, asigurându-se că modelul nu uită ce știa deja în timp ce învață lucruri noi

<Tip>

Inovațiile cheie ale GRPO sunt:
- Învățarea direct din orice funcție sau model, eliminând dependența de un model de recompensă separat.
- Învățarea bazată pe grup, care este mai stabilă și eficientă decât metodele tradiționale cum ar fi comparațiile perechi.

</Tip>

Această descompunere este complexă, dar concluzia cheie este că GRPO este o modalitate mai eficientă și stabilă de a antrena un model să raționeze.

### Algoritmul GRPO în Pseudocod

Acum că înțelegem componentele cheie ale GRPO, să privim algoritmul în pseudocod. Aceasta este o versiune simplificată a algoritmului, dar surprinde ideile cheie.

```
Input: 
- initial_policy: Modelul de început care va fi antrenat
- reward_function: Funcția care evaluează rezultatele
- training_prompts: Set de exemple de antrenare
- group_size: Numărul de rezultate per comandă (de obicei 4-16)

Algoritm GRPO:
1. Pentru fiecare iterație de antrenare:
   a. Setează reference_policy = initial_policy (captura politica actuală)
   b. Pentru fiecare comandă în lot:
      i. Generează group_size rezultate diferite folosind initial_policy
      ii. Calculează recompensele pentru fiecare rezultat folosind reward_function
      iii. Normalizează recompensele în cadrul grupului:
           normalized_advantage = (recompensă - mean(recompense)) / std(recompense)
      iv. Actualizează politica maximizând rația tăiată:
          min(prob_ratio * normalized_advantage, 
              clip(prob_ratio, 1-epsilon, 1+epsilon) * normalized_advantage)
          - kl_weight * KL(initial_policy || reference_policy)
          
          unde prob_ratio este current_prob / reference_prob

Output: Model de politică optimizat
```

Acest algoritm arată cum GRPO combină estimarea avantajului bazat pe grup cu optimizarea politicii menținând în același timp stabilitatea prin tăiere și constrângeri de divergență KL. 