<FrameworkSwitchCourse {fw} />

# Fine-tuning, Check![[fine-tuning-check]]

<CourseFloatingBanner
    chapter={3}
    classNames="absolute z-10 right-0 top-0"
/>

That was comprehensive! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data using modern best practices. To recap, in this chapter you:

* Learned about datasets on the [Hub](https://huggingface.co/datasets) and modern data processing techniques
* Learned how to load and preprocess datasets efficiently, including using dynamic padding and data collators
* Implemented fine-tuning and evaluation using the high-level `Trainer` API with the latest features
* Implemented a complete custom training loop from scratch with PyTorch
* Used ðŸ¤— Accelerate to easily adapt your training loop for distributed training on multiple GPUs or TPUs
* Explored modern optimization techniques and training best practices
* Learned about proper model evaluation using the ðŸ¤— Evaluate library

**Key takeaways for modern fine-tuning:**

- Always use the latest APIs like `processing_class` in the Trainer
- Consider mixed precision training and gradient accumulation for efficiency
- Use the ðŸ¤— Evaluate library for comprehensive model evaluation
- Leverage ðŸ¤— Accelerate for seamless distributed training
- Apply modern optimization techniques like proper learning rate scheduling and weight decay
- Monitor your training with proper metrics and validation

You now have all the tools needed to fine-tune state-of-the-art language models effectively using the latest best practices from the Hugging Face ecosystem!
