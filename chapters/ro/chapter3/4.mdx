# O antrenare completă [[a-full-training]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[ 
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"}, 
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section4.ipynb"}, 
]} />

<Youtube id="Dh9CL8fyG80"/>

Acum vom vedea cum putem atinge același rezultat pe care l-am obținut în secțiune anterioară fără a utiliza clasa `Trainer`. Din nou, noi presupunem că ai procesat datele în secțiunea 2. În continuare, găsiți o scurtă sinteză asupra tuturor informațiilor de care aveți nevoie:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Pregătirea pentru antrenare[[prepare-for-training]]

Înainte de a scrie propriul nostru loop de antrenament, vom avea nevoie să definim câteva obiecte. Primele dintre ele sunt dataloader-urile pe care le vom utiliza pentru a itera peste batch-uri. Dar înainte de a putea defini aceste datealoader-uri, trebuie să aplicăm o anumită postprocesare pe `tokenized_datasets`, pentru a avea grijă de unele lucruri care au fost automatizate de către `Trainer`. În special:

- Eliminăm coloanele corespunzătoare valorilor pe care modelul nu le așteaptă (ca de exemplu, `sentence1` și `sentence2`).
- Redenumește coloana `label` în `labels`, deoarece modelul așteaptă argumentul să fie numit `labels`.
- Setăm formatul dataseturilor astfel încât ele să returneze tensore PyTorch în loc de liste.

Metodele noastre `tokenized_datasets` au fiecare unul dintre acești pași:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

Vom verifica că rezultatul conține doar coloanele pe care vor modelul le acceptă:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

Acum că am făcut asta, putem defini cu ușurință dataloader-urile noastre:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

Pentru a verifica rapid că nu există nici o greșeală în procesarea datelor, putem inspecta un batch-ul astfel:

```py
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

Observați că shapeurile actuale vor fi probabil ușor diferite pentru tine, deoarece am setat `shuffle=True` pentru dataloader-ul de antrenare și am făcut padding batch-urilor la lungime maximă în interiorul lor.

Acum că am terminat complet procesarea datelor (un obiectiv satisfăcător, dar înșelător pentru orice specialist ML), să trecem la model. Îl inițializăm exact ca în secțiunea precedentă:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Pentru a ne asigura că totul va merge bine în timpul antrenamentului, vom transmite batch-ul nostru modelului acesta:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

Toate modelele 🤗 Transformers vor returna lossul atunci când `labels` sunt furnizate și vom primi de asemenea logiturile (două pentru fiecare input în batch-ul nostru, deci un tensor cu dimensiunea 8 x 2).

Suntem aproape gata să scriem loopul nostru de antrenament! Doar că ne lipsesc două lucruri: un optimizer și learning rate scheduler. Deoarece încercăm să replicăm ceea ce a făcut `Trainer` manual, vom folosi aceleași argumente default. Optimizerul utilizat de `Trainer` este `AdamW`, care este similar cu Adam, dar cu un twist pentru weight decay regularization (vedeți ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) de Ilya Loshchilov și Frank Hutter):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

La final, learning rate schedulerul utilizat este un linear decay de la valoarea maximă (5e-5) la 0. Pentru a-l defini cu adevărat, avem nevoie să cunoaștem numărul de pași de antrenament pe care îi vom face, care este numărul de epoci pe care dorim să le rulăm înmulțit cu numărul de batch-uri de antrenare (care este lungimea dataloader-ului de antrenare). `Trainer` folosește trei epoci ca default, deci vom continua cu asta:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### Training Loop[[the-training-loop]]

Un ultim lucru: vom dori să folosim un GPU dacă avem acces la unul (pe un CPU, instruirea poate dura câteva ore în loc de câteva minute). Pentru a face acest lucru, definim un `device` pe care vom plasa modelul și batch-urile noastre:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

Suntem acum gata pentru antrenare! Pentru a avea o idee când va fi finalizată antrenarea, adăugăm un progress bar peste numărul nostru de pași de antrenare, folosind biblioteca `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Poți să vezi că nucleul loopului de instruire are o formă similară cu cea din introducere. Nu am cerut nicio raportare, deci acest loop de antrenare nu ne va spune nimic despre cum se descurcă modelul. Trebuie să adăugăm un evaluation loop pentru a face acest lucru.


### Evaluation Loop[[the-evaluation-loop]]

La fel ca înainte, vom folosi o metrică oferită de librăria 🤗 Evaluate. Am văzut deja metoda `metric.compute()`, dar metricile pot acumula batch-urile pentru noi pe măsură ce mergem peste loopurile de  predicțoie cu metoda `add_batch()`. Când am acumulat toate batch-urile, putem să obținem rezultatul final cu `metric.compute()`. Acesta este modul în care trebuie să implementăm acest lucru într-un loop de evaluare:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

Rezultatele tale vor fi ușor diferite datorită randomizării în inițializarea headului modelului și a data shuffling, dar ele nu ar trebui să fie foart diferite.


<Tip>

✏️ **Încearcă!** Modifica loopul de antrenare dinainte pentru a face fine-tune modelul pe dataset-ul SST-2.

</Tip>


### Supercharge Training Loopul cu 🤗 Accelerate[[supercharge-your-training-loop-with-accelerate]]

<Youtube id="s7dy8QRgjJ0" />

Loopul de antrenare pe care l-am definit anterior funcționează bine pe un singur CPU sau GPU. Dar folosind libraria [🤗 Accelerate](https://github.com/huggingface/accelerate), cu câteva ajustări putem activa distributed training pe multiple GPU-uri sau TPU-uri. Începând de la crearea training și validation dataloaders, aceasta este este loopul manual de antrenare:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Și aici sunt schimbările:

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

Prima linie de adăugat este importarea librăriei. A doua linie creează un obiect `Accelerator` care va evalua environmentul și va inițializa proper distributed setup. 🤗 Accelerate gestionează automat poziționarea deviceului pentru tine, așa că poți șterge rândurile care pun modelul pe device (sau dacă preferi, poți să le schimbi cu `accelerator.device` în loc de `device`). 

Partea principală a lucrului se face în linia ce trimite dataloader-urile, modelul și optimizerul la `accelerator.prepare()`. Acaeasta va face wrap acestor obiecte în containerul potrivit pentru a asigura o antrenare distribuită corespunzătoare. Ultimele schimbări sunt ștergerea linei cep une batch-ul pe device(din nou, dacă vvrei să lași acest lucru poți să le schimbi cu `accelerator.device`) și schimbarea `loss.backward()` cu `accelerator.backward(loss)`.

<Tip>
⚠️ Pentru a beneficia de viteza oferită de Cloud TPUs, recomandăm să faceți padding sampleurilor la o lungime fixă folosind argumentele `padding="max_length"` și `max_length` ale tokenizerului.
</Tip>

Dacă vrei să copiezi codul pentru a-l testa, aici este loopul complet de antrenare cu Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

Scriind în `train.py` aceste modificări, vor face scriptul executabil pe orice tip de distributed setup. Pentru a testa codul în mediul tău distribuit, rulează următoarea comandă:

```bash
accelerate config
```

Ceea ce îți va oferi să răspunzi la o serie de întrebări și să salvezi răspunsurile într-un fișier de configurare folosit de acest modul: 

```bash
accelerate launch train.py
```

Această comandă va lansa loopul de antrenare pe dispozitivele distribuite.

Dacă doriți să încercați acest lucru într-un Jupyter Notebook (de exemplu, pentru a testa TPU-urile de pe Colab), înlocuiți codul cu următoarea funcție:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

Pentru mai multe exemple consultați [repo-ul 🤗 Accelerate](https://github.com/huggingface/accelerate/tree/main/examples).