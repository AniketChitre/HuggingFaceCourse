
<FrameworkSwitchCourse {fw} />

# Fine-tuningul unui model cu API-ul Trainer[[fine-tuning-a-model-with-the-trainer-api]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[ 
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter3/section3.ipynb"},
]} />

<Youtube id="nvBXf7s7vTI"/>

🤗 Transformers oferă o clasă `Trainer` pentru a vă ajuta să faceți fine-tune pe oricare dintre modelurile preantrenate pe care le oferă pe datasetul dvs. Odată ce ați terminat preprocesarea datelor din ultima secțiune, mai aveți doar câteva pași rămași pentru a defini `Trainerul`. Partea cea mai grea este probabil pregătirea environmentul pentru a rula `Trainer.train()`, deoarece va lua mult timp pe un CPU. Dacă nu aveți niciun GPU configurat, puteți accesa gratuit GPUuri sau TPUuri pe [Google Colab](https://colab.research.google.com/).

Exemplele de cod de mai jos presupune că ați executat exemplele din secțiunea anterioară. Aici este o scurtă recapitulare despre ce aveți nevoie:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### Antrenarea[[training]]

Înainte de a putea defini `Trainerul`, trebuie să definim o clasă `TrainingArguments` care va conține toți hyperparameters pe care `Trainer` le va folosi pentru antrenare și evaluare. Singurul argument pe care trebuie să-l oferiți este un folder în care modelul anternat va fi salvat, precum și checkpointurile de-a lungul drumului. Toate celelalte pot fi lăsate ca valori default, care ar trebui să funcționeze destul de bine pentru un fine-tune de bază.

```py
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<Tip>

💡 Dacă doriți să încărcați automat modelul pe Hub în timpul antrenării, transmiteți `push_to_hub=True` în `TrainingArguments`. Ne vom întoarce la acest subiect în [Capitolul 4](/course/chapter4/3)

</Tip>

A doua etapă este definiția modelului nostru. Ca în capitolul anterior, vom folosi clasa `AutoModelForSequenceClassification`, cu două labeluri:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Observăm că, spre deosebire de [Capitolul 2](/course/chapter2), se emite o avertizare după crearea acestui model preantrenat. Acest lucru este datorit faptului că BERT nu a fost antrenat pentru clasificarea perechilor de propoziții, astfel încât începutul modelului preantrenat a fost eliminat și un nou început adecvat pentru clasificarea secvențelor a fost adăugat în loc. Avertizările indică faptul că anumite weights nu au fost utilizate (cele care corespundeau începutul eliminat) și celelalte au fost inițializate aleatoriu (cele pentru noul început). Aceasta se termină cu o recomandare de a antrena modelul, ceea ce vom face acum.

Odată ce am definit modelul nostru, putem defini `Trainer` prin transmiterea tuturor obiectelor construite până acum — `model`, `training_args`, a training și validation datasets, `data_collator`, și `tokenizer`:

```py
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

Observăm că, dacă transmitem `tokenizer`, atunci defaultul `data_collator` folosit de `Trainer` va fi un `DataCollatorWithPadding` similar celui definit mai devreme. Din acest motiv, puteți omite linia `data_collator=data_collator` aici. A fost important să vă arătăm această parte în secțiunea 2!

Pentru a face fine-tune modelului pe datasetul nostru, trebuie să apelăm metoda `train()` a `Trainerului`:

```py
trainer.train()
```

Acest lucru va începe fine-tuningul (care ar trebui să dureze câteva minute pe un GPU) și va raporta training loss la fiecare 500 de pași. Totuși, nu va raporta cât de bine sau rău se descurcă modelul. Acest lucru este datorat:

1. Nu am transmis `Trainerului` să efectueze evaluarea în timpul antrenării, prin setarea `evaluation_strategy` la `"steps"` (evaluați la fiecare `eval_steps`) sau `"epoch"` (evaluați la finalul fiecărei epoch).
2. Nu am oferit `Trainerului` o funcție `compute_metrics()` pentru a calcula metricele în timpul evaluări(altminter evalurea ar fi printat doar lossul, care nu este un număr foarte intuitiv).

### Evaluare[[evaluation]]

Să vedem cum putem construi o funcție `compute_metrics()` folositoare și să o utilizăm la următoarea antrenare. Funcția trebuie să primească obiectul `EvalPrediction` (un named tuple cu fieldul `predictions` și altul `label_ids`) și să returneze un dicționar ce le asociază făcând mapping valorilor string la valori float (stringurile fiind denumirile metricelor returnate, și valorile floats al acestora). Pentru a obține câteva predicții din model, putem folosi comanda `Trainer.predict()`:

```py
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python out
(408, 2) (408,)
```

Outputul metodei `predict()` este un named tuple  cu trei fielduri: `predictions`, `label_ids` și `metrics`. Câmpul `metrics` va conține doar lossul asupra datasetului transmis, precum și o serie de metrice de timp (cât de mult a luat prezicerea și timpul mediu). Odată ce vom completa funcția `compute_metrics()` și îl vom oferi `Trainerului`, atunci acel field va conține și metricele returnate de `compute_metrics()`.

După cum puteți vedea, `predictions` este un array bi-dimensional cu shapeul 408x2 (408 fiind numărul de elemente în datasetul folosit). Acestea sunt logiturile pentru fiecare element al datasetului pe care le-am oferit funcției `predict()` (cum ați văzut în [capitolul anterior](/course/chapter2), toate modelel Transformer returneă logituri). Pentru a le transforma în predicții pe care să le comparăm cu labelurile noastre, noi trebui să luăm indexul cu cea mai mare valoare pe axa a doua:


```py
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

Acum putem compara `preds` cu labelurile. Ca să construim funcțioa noastră `compute_metric()`, o să ne bazăm pe metricele de la librăria 🤗 [Evaluate](https://github.com/huggingface/evaluate/). Putem încărca metricele asociate cu datasetul MRPC la fel de ușor cum am încărcat datasetul, de data asta cu funcția `evaluate.load()`. Obiectul returnat are o metodă `compute()` pe care o putem folosi ca să facem calcularea metricelor:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=np.argmax(predictions.predictions, axis=-1), references=predictions.label_ids)
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

Rezultatele exacte pe care le veți obține pot varia, deoarece inițializarea aleatoare a începutului modelului care poate schimba metricele pe care le-a atins. Aici putem vedea că modelul nostru are o precizie de 85.78% în setul de validare și un scor F1 de 89.97. Acestea sunt exact aceleași metrice folosite pentru evaluarea rezultatelor pe datasetul MRPC pentru GLUE benchmark. Tabelul din [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf) a raportat un scor F1 de 88.9 pentru modelul de bază. Acela a fost un `uncased` model, dar noi utilizăm acum un `cased` model, ceea ce explică rezultatul mai bun.

Pentru a reuni toate acestea într-o singură funcție `compute_metrics()`, putem scrie:

```py
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Să vedem acum cum putem utiliza această funcție pentru a raporta metricele la sfârșitu fiecărei epoch, mai jos puteți vedea cum definim un nou `Trainer` cu funcția compute:

```py
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Observăm că am creat un nou `TrainingArguments` cu `evaluation_strategy` setat la `"epoch"` și o nouă instanță a modelului. În cazul nostru, ar fi fost suficient să continuam antrenarea prezentată anterior.

Pentru a lansa o nouă antrenare, putem executa:

```py
trainer.train()
```

De data aceasta, acesta va raporta validation loss și metricele la sfârșitul fiecărui epocă pe lângă training loss. Dar acuratețea/scorul F1 pe care îl atingeți poate fi puțin diferit față de ceea ce am găsit, datorită inițializării aleatoare a începutului modelului, dar ar trebui să nu difere foarte mult.

`Trainerul` va funcționa în mod automat pe mai multe GPU-uri sau TPU-uri și oferă multe opțiuni, cum ar fi mixed-precision training(folosiți `fp16 = True` în argumentele de antrenare). Vom discuta despre toate opțiunile pe care le are în Capitolul 10.

Cu aceasta terminăm introducerea fine-tuningului folosind API-ul `Trainer`. Un exemplu de a face acest lucru pentru majoritatea sarcinilor NLP va fi dat în [Capitolul 7](/course/chapter7), dar pentru moment să vedem cum putem face același lucru doar cu PyTorch.

<Tip>

✏️ **Încearcă!** Fă fine-tune unui model pe datasetul GLUE SST-2, folosind procesarea de date efectuată în secțiunea 2.

</Tip>
