# Înțelegerea NLP[[mastering-nlp]]

<CourseFloatingBanner
    chapter={7}
    classNames="absolute z-10 right-0 top-0"
/>

Dacă ați ajuns până aici în curs, felicitări - acum aveți toate cunoștințele și instrumentele necesare pentru a aborda (aproape) orice sarcină NLP cu 🤗 Transformers și ecosistemul Hugging Face!

Am văzut o mulțime de data collators, așa că am făcut acest mic videoclip pentru a vă ajuta să găsiți cel pe care să îl utilizați pentru fiecare sarcină:

<Youtube id="-RPeakdlHYo"/>

După finalizarea acestui tur fulger prin sarcinile de bază ale NLP, ar trebui să:

* Știți care arhitecturi (codificator, decodificator sau codificator-decodificator) sunt cele mai potrivite pentru fiecare sarcină
* Înțelegeți diferența dintre preantrenare și fine-tuningul a unui model lingvistic
* Știți cum să antrenați modele Transformer utilizând API-ul `Trainer` și caracteristicile de antrenare distribuită ale 🤗 Accelerate sau TensorFlow și Keras, în funcție de calea pe care ați urmat-o
* Înțelegeți semnificația și limitele metricilor precum ROUGE și BLEU pentru sarcinile de generare a textului
* Știți cum să interacționați cu modelele dvs. ajustate, atât pe Hub, cât și utilizând `pipeline` din 🤗 Transformers

În ciuda tuturor acestor cunoștințe, va veni un moment în care fie veți întâlni un bug dificil în codul vostru fie veți avea o întrebare despre cum să rezolvați o anumită problemă NLP. Din fericire, comunitatea Hugging Face este aici pentru a vă ajuta! În ultimul capitol al acestei părți a cursului, vom explora modul în care vă puteți face debugging modelelor Transformer și solicita ajutor în mod eficient.